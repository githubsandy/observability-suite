# Kubernetes Master-Worker Architecture & Helm Deployment Guide

## ğŸ¯ **Document Purpose**
This document explains how Kubernetes master-worker architecture works and what happens behind the scenes when we deploy our Splunk OpenTelemetry solution using Helm.

---

## ğŸ—ï¸ **Kubernetes Cluster Architecture Overview**

### **Your Current Environment**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Kubernetes Cluster                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        CONTROL PLANE        â”‚                 WORKER NODES                      â”‚
â”‚    (uta-k8s-ctrlplane-01)   â”‚                                                   â”‚
â”‚     IP: 10.122.28.111       â”‚   â€¢ uta-k8s-ao-01 (observability nodes)         â”‚
â”‚     User: administrator     â”‚   â€¢ uta-k8s-ao-02                                â”‚
â”‚     Pass: C1sco123=         â”‚   â€¢ uta-k8s-worker-01 to 11 (application nodes)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ§  CONTROL COMPONENTS:     â”‚  ğŸ’ª WORKER COMPONENTS:                           â”‚
â”‚  â€¢ API Server               â”‚  â€¢ kubelet (node agent)                          â”‚
â”‚  â€¢ etcd (cluster database)  â”‚  â€¢ Container runtime (containerd)                â”‚
â”‚  â€¢ Scheduler                â”‚  â€¢ kube-proxy (networking)                       â”‚
â”‚  â€¢ Controller Manager       â”‚  â€¢ Your applications run here                    â”‚
â”‚  â€¢ kubectl/helm access      â”‚  â€¢ OTEL collectors will run here                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ­ **Roles & Responsibilities**

### **Control Plane (Master) - "The Brain"**
| Component | Purpose | What It Does |
|-----------|---------|--------------|
| **API Server** | Central communication hub | Receives all kubectl/helm commands |
| **etcd** | Cluster database | Stores desired state of all resources |
| **Scheduler** | Pod placement decision maker | Decides which worker node runs each pod |
| **Controller Manager** | State reconciliation | Ensures actual state matches desired state |

**Your Role:** You SSH here and run management commands (kubectl, helm)

### **Worker Nodes - "The Muscle"**
| Component | Purpose | What It Does |
|-----------|---------|--------------|
| **kubelet** | Node agent | Communicates with control plane, manages pods |
| **Container Runtime** | Pod execution | Actually runs containers (containerd in your case) |
| **kube-proxy** | Networking | Handles service networking and load balancing |

**Their Role:** Execute actual workloads, run your CXTM applications and OTEL collectors

---

## ğŸš€ **Helm Deployment Process - Step by Step**

### **What Happens When You Run `./deploy-helm-otel.sh`**

#### **Phase 1: Command Execution (Control Plane)**
```
You (SSH to control plane) â†’ Execute ./deploy-helm-otel.sh
                           â†“
                    Helm client starts
```

#### **Phase 2: Helm Processing (Control Plane)**
```
1. Helm reads helm-values.yaml
2. Helm downloads Splunk OTEL chart from repository
3. Helm renders Kubernetes YAML manifests
4. Helm sends manifests to API Server
```

#### **Phase 3: API Server Processing (Control Plane)**
```
API Server receives requests â†’ Validates manifests â†’ Stores in etcd
                            â†“
                    Creates resource objects:
                    â€¢ Namespace: splunk-monitoring
                    â€¢ Deployment: opentelemetry-operator
                    â€¢ DaemonSet: splunk-otel-collector-agent
                    â€¢ ConfigMaps, Services, etc.
```

#### **Phase 4: Scheduler Decision Making (Control Plane)**
```
Scheduler watches for unscheduled pods â†’ Analyzes resource requirements
                                       â†“
                            Decides placement based on:
                            â€¢ Node capacity (CPU, memory)
                            â€¢ Node selectors/taints
                            â€¢ Anti-affinity rules
                            â€¢ Resource constraints
```

#### **Phase 5: Pod Creation (Worker Nodes)**
```
kubelet on selected worker nodes â†’ Receives pod specifications
                                 â†“
                          Pulls container images
                                 â†“
                          Creates and starts containers
```

---

## ğŸ“Š **Deployment Distribution Across Nodes**

### **What Gets Deployed WHERE**

#### **OpenTelemetry Operator (Deployment)**
- **Type**: Single pod deployment
- **Placement**: Scheduler picks ONE worker node
- **Purpose**: Manages auto-instrumentation for applications
- **Resource**: Lightweight (usually < 100MB)

```
Control Plane Decision:
"Deploy operator on worker-03 (has available resources)"
```

#### **OpenTelemetry Collector (DaemonSet)**
- **Type**: One pod per worker node
- **Placement**: ALL worker nodes (16 pods total)
- **Purpose**: Collect traces/metrics from applications on each node
- **Resource**: ~200MB per node

```
Control Plane Decision:
"Deploy collector pod on EVERY worker node"

Result:
uta-k8s-ao-01      â†’ splunk-otel-collector-agent-xxxxx
uta-k8s-ao-02      â†’ splunk-otel-collector-agent-xxxxx
uta-k8s-worker-01  â†’ splunk-otel-collector-agent-xxxxx
uta-k8s-worker-02  â†’ splunk-otel-collector-agent-xxxxx
... (and so on for all 16 nodes)
```

#### **CXTM Applications (Already Running)**
- **Current State**: Distributed across worker nodes
- **After Instrumentation**: Same nodes, but with added init containers

```
Current Distribution:
uta-k8s-worker-01  â†’ cxtm-web-xxx
uta-k8s-worker-03  â†’ cxtm-scheduler-xxx  
uta-k8s-worker-07  â†’ cxtm-zipservice-xxx
... (existing placement)
```

---

## ğŸ”„ **Communication Flow During Runtime**

### **Trace Collection Flow**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CXTM App      â”‚â”€â”€â”€â”€â”‚  OTEL Collector â”‚â”€â”€â”€â”€â”‚   Splunk O11y   â”‚
â”‚ (worker-01)     â”‚    â”‚  (worker-01)    â”‚    â”‚     Cloud       â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ Flask app       â”‚    â”‚ Same node       â”‚    â”‚ External SaaS   â”‚
â”‚ generates       â”‚    â”‚ collects &      â”‚    â”‚ displays APM    â”‚
â”‚ trace spans     â”‚    â”‚ forwards        â”‚    â”‚ data            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚ HTTP/gRPC            â”‚ HTTPS                 â”‚
         â”‚ localhost:4318       â”‚ ingest.us1.signalfx.com
         â”‚                      â”‚                       â”‚
    In-node communication    Internet communication
```

### **Management Flow**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   You (SSH)     â”‚â”€â”€â”€â”€â”‚  Control Plane  â”‚â”€â”€â”€â”€â”‚  Worker Nodes   â”‚
â”‚ kubectl/helm    â”‚    â”‚  API Server     â”‚    â”‚  kubelet        â”‚
â”‚ commands        â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚                 â”‚    â”‚ Processes       â”‚    â”‚ Executes        â”‚
â”‚                 â”‚    â”‚ requests        â”‚    â”‚ instructions    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚ API calls            â”‚ Instructions          â”‚
         â”‚ Port 6443            â”‚ Various protocols     â”‚
```

---

## ğŸ¯ **Key Concepts Explained**

### **Why DaemonSet for OTEL Collector?**
- **Goal**: Collect metrics/traces from ALL nodes
- **Solution**: DaemonSet ensures one collector per node
- **Benefit**: Local collection (low latency, no network overhead)

### **Why Deployment for Operator?**
- **Goal**: Manage instrumentation across cluster
- **Solution**: Single operator can manage all nodes
- **Benefit**: Centralized control, resource efficient

### **Why Init Containers for Instrumentation?**  
- **Problem**: Runtime pip install was unreliable
- **Solution**: Init container pre-installs libraries
- **Benefit**: Libraries ready before application starts

---

## ğŸ” **Monitoring & Troubleshooting**

### **Commands You Run (Control Plane)**
```bash
# Check what's deployed where
kubectl get pods -o wide -n splunk-monitoring

# See which node runs which pod
kubectl get pods -n cxtm -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName

# Check logs from any worker node
kubectl logs -l app.kubernetes.io/name=splunk-otel-collector -n splunk-monitoring

# Describe resources on any node
kubectl describe pod <pod-name> -n splunk-monitoring
```

### **What Happens Behind the Scenes**
1. **kubectl** sends API request to control plane
2. **API Server** retrieves information from etcd
3. **If logs requested**: API Server fetches from kubelet on worker node
4. **Response** flows back through control plane to your terminal

---

## ğŸ’¡ **Advantages of This Architecture**

### **Scalability**
- **Control Plane**: Handles cluster management for any number of nodes
- **Worker Nodes**: Can add more nodes without changing control plane

### **Reliability** 
- **Control Plane**: Can be made highly available (3 masters in your case)
- **Worker Nodes**: If one fails, workloads can be rescheduled to others

### **Security**
- **Control Plane**: Centralized access control and authentication
- **Worker Nodes**: Isolated workload execution

### **Observability**
- **Control Plane**: Central monitoring and logging of cluster state
- **Worker Nodes**: Distributed data collection (OTEL collectors)

---

## ğŸš¨ **Common Misconceptions Clarified**

### **âŒ Misconception**: "I need to SSH to worker nodes to deploy"
**âœ… Reality**: You only SSH to control plane. Kubernetes handles worker node operations.

### **âŒ Misconception**: "Applications run on control plane"  
**âœ… Reality**: Applications run on worker nodes. Control plane only manages them.

### **âŒ Misconception**: "Each node needs separate configuration"
**âœ… Reality**: One configuration applies to all nodes via Kubernetes controllers.

### **âŒ Misconception**: "I need to manually start services on each node"
**âœ… Reality**: Kubernetes automatically ensures desired state across all nodes.

---

## ğŸ¯ **Practical Example: Your CXTM Deployment**

### **Before Helm Deployment**
```
Control Plane: You manage cluster, CXTM apps run on workers
Worker Nodes:  cxtm-web, cxtm-scheduler, etc. (no observability)
```

### **During Helm Deployment**  
```
Control Plane: Processes Helm charts, schedules new pods
Worker Nodes:  Kubernetes starts OTEL collectors and operator
```

### **After Helm Deployment**
```
Control Plane: Monitors health, manages configurations
Worker Nodes:  CXTM apps + OTEL collectors collecting traces
Splunk O11y:   Receives and displays application performance data
```

---

## ğŸ”„ **Summary: Your Role vs Kubernetes Role**

### **Your Role (Control Plane)**
- âœ… Execute deployment commands
- âœ… Monitor cluster health  
- âœ… Troubleshoot issues
- âœ… Configure resources

### **Kubernetes Role (Automated)**
- âœ… Schedule pods to appropriate nodes
- âœ… Ensure desired state is maintained
- âœ… Handle pod restarts and failures
- âœ… Manage networking and storage

### **Worker Nodes Role (Automated)**
- âœ… Run assigned workloads
- âœ… Collect and forward observability data
- âœ… Report status back to control plane
- âœ… Execute container operations

---

This architecture ensures that your **single command execution** on the control plane results in **coordinated deployment across all 16 worker nodes**, with **automatic management and monitoring** of the entire observability pipeline! ğŸš€