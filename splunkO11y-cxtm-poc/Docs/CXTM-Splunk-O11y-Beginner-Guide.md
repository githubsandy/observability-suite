# CXTM Splunk Observability - Simple Working Guide

## ğŸ¯ What This Guide Does

**Goal**: Get CXTM applications visible in Splunk Observability Cloud in 15 minutes!

**Result**: Complete application monitoring with service maps, performance metrics, and alerts.

## âœ… What Actually Works (The Simple Solution)

After 2 days of testing, here's the **simple approach that actually works**:

1. âœ… **Use existing Splunk infrastructure** (don't deploy new collectors)
2. âœ… **Set environment variable directly** (bypass complex configurations)  
3. âœ… **Add OpenTelemetry annotation** (enable auto-instrumentation)
4. âœ… **Test and verify** (generate traffic and check dashboard)

**Time to success**: 15 minutes  
**Complexity**: Low  
**Reliability**: High âœ…

---

## ğŸ—ï¸ Understanding the Setup (Namespace Scoping Made Simple)

### What are Namespaces?
Think of Kubernetes namespaces like separate "folders" or "rooms" in our cluster:
- **`ao` namespace** = Infrastructure monitoring room (where Preeti's work lives)
- **`cxtm` namespace** = CXTM applications room (where our apps live)

### Key Rule: **Namespace Scoping**
âš ï¸ **IMPORTANT**: Instrumentation only works within the same namespace!
- If we put instrumentation config in `ao` namespace â†’ only `ao` apps get instrumented
- If we put instrumentation config in `cxtm` namespace â†’ only `cxtm` apps get instrumented

### Our Setup Layout
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ao namespace (Infrastructure - Preeti's)    â”‚
â”‚ â”œâ”€â”€ splunk-otel-collector (data collection) â”‚
â”‚ â”œâ”€â”€ opentelemetry-operator (automation)     â”‚
â”‚ â””â”€â”€ splunk-instrumentation (config)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†‘ 
          â”‚ (sends data)
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ cxtm namespace (Our Applications)           â”‚
â”‚ â”œâ”€â”€ cxtm-web (our main app)                 â”‚
â”‚ â”œâ”€â”€ cxtm-python-instrumentation (our config)â”‚
â”‚ â””â”€â”€ Other CXTM services                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Simple 4-Step Solution (What Actually Works)

### Step 1: Check Existing Infrastructure (30 seconds)
```bash
# SSH to CALO lab machine
ssh administrator@10.122.28.111
# Password: C1sco123=

# Check if Splunk collector is already running
kubectl get pods -n ao | grep splunk-otel-collector
```

**Expected result**: You should see collector pods running âœ…  
**Why this works**: Use existing infrastructure instead of deploying new components

---

### Step 2: Set Environment Variable (30 seconds)
```bash
# Set the correct collector endpoint directly on your app
kubectl set env deployment cxtm-web -n cxtm \
  OTEL_EXPORTER_OTLP_ENDPOINT=http://splunk-otel-collector-agent.ao.svc.cluster.local:4318
```

**Why this works**: Bypasses complex configuration files that often get overridden

---

### Step 3: Enable Auto-Instrumentation (30 seconds)
```bash
# Add the annotation to enable OpenTelemetry
kubectl annotate deployment cxtm-web -n cxtm \
  instrumentation.opentelemetry.io/inject-python=cxtm-python-instrumentation

# Set service name for identification
kubectl set env deployment cxtm-web -n cxtm OTEL_SERVICE_NAME=cxtm-web
```

**Expected result**: Pod will restart with OpenTelemetry libraries injected

---

### Step 4: Test and Verify (5 minutes)
```bash
# Wait for pod to be ready
kubectl get pods -n cxtm | grep cxtm-web
# Should show: 1/1 Running

# Generate test traffic
kubectl port-forward -n cxtm service/cxtm-web 8082:8080 &
curl http://localhost:8082/healthz
curl http://localhost:8082/

# Check Splunk dashboard
# Go to: https://cisco-cx-observe.signalfx.com
# Look for: cxtm-web service in APM
```

**Expected result**: Service visible with metrics, traces, and service map âœ…

---

## ğŸ” Understanding What Happens (The Magic Explained)

### Before Instrumentation
```
User Request â†’ cxtm-web â†’ Response
(No visibility into what happened)
```

### After Instrumentation  
```
User Request â†’ cxtm-web â†’ Redis â†’ MySQL â†’ Response
     â†“           â†“         â†“       â†“        â†“
   Trace     Web Span  Cache    DB      Response
   Created   Started   Span     Span    Completed
     â†“           â†“         â†“       â†“        â†“
        All spans sent to Splunk O11y Cloud
```

### The Flow in Detail
1. **Request comes in** â†’ OpenTelemetry creates a trace ID
2. **cxtm-web processes** â†’ Creates spans for each operation
3. **Database calls happen** â†’ Automatically tracked as separate spans  
4. **Cache operations** â†’ Also tracked automatically
5. **All span data** â†’ Sent to collector in `ao` namespace
6. **Collector forwards** â†’ Data goes to Splunk Cloud
7. **We see results** â†’ In Splunk Observability dashboard!

---

## ğŸ§  Lessons Learned from Our 2-Day Journey

### Yesterday's Key Discoveries
**What we learned the hard way:**

1. **OpenTelemetry Operator Webhook Issues**
   - **Problem**: Webhook couldn't process pod mutations during startup
   - **Symptoms**: Pods restarted but no init containers appeared
   - **Learning**: Sometimes operator webhooks have timing dependencies

2. **Infrastructure vs Application Separation**
   - **Problem**: Tried to deploy everything from scratch
   - **Learning**: Check if infrastructure already exists first!
   - **Solution**: Collaborate with existing deployments (Preeti's work)

3. **Manual vs Operator Approaches**
   - **Problem**: Manual `opentelemetry-instrument` wrapper in runtime commands failed
   - **Symptoms**: Libraries installed but Flask auto-instrumentation didn't work
   - **Learning**: Gunicorn + Flask + runtime pip install = complex conflicts

### Today's Breakthrough Insights

1. **Namespace Scoping is Everything**
   - **Key insight**: Instrumentation CRD must be in same namespace as target apps
   - **Yesterday's mistake**: Trying to use cross-namespace instrumentation incorrectly
   - **Today's fix**: Created `cxtm-python-instrumentation` in `cxtm` namespace

2. **Existing Infrastructure is Your Friend**
   - **Discovery**: Preeti had already deployed collector and operator perfectly
   - **Approach**: Work WITH existing setup, not against it
   - **Result**: No conflicts, faster deployment

3. **Container Startup Commands Matter**
   - **Problem**: Operator injected libraries but didn't modify startup command
   - **Diagnosis**: `ps aux` showed app running without OpenTelemetry wrapper
   - **Solution**: Manual deployment patch to wrap with `opentelemetry-instrument`

4. **File Paths and Working Directories**
   - **Crash**: `gunicorn_conf.py doesn't exist`
   - **Root cause**: Wrong working directory in container
   - **Fix**: Set `workingDir: /app` and correct config path `src/gunicorn_conf.py`

---

## ğŸ› ï¸ Common Issues and Simple Fixes (From Real Experience)

### Issue 1: "No services showing in Splunk"
**Cause**: Namespace scoping problem  
**Check**: Are instrumentation and apps in the same namespace?
```bash
kubectl get instrumentation -n cxtm  # Should show our config
kubectl get pods -n cxtm             # Should show our apps
```

### Issue 2: "App keeps crashing"
**Cause**: Wrong file paths or working directory  
**Fix**: Check where files actually are:
```bash
kubectl exec <pod-name> -n cxtm -- pwd
kubectl exec <pod-name> -n cxtm -- ls -la
```

### Issue 3: "OpenTelemetry not starting"
**Cause**: App started without wrapper  
**Check**: Is the startup command correct?
```bash
kubectl describe pod <pod-name> -n cxtm | grep -A10 "Command"
kubectl exec <pod-name> -n cxtm -- ps aux | head -5
```

---

## ğŸ“Š How to Verify It's Working

### Step 1: Check the Basics
```bash
# 1. Instrumentation exists in correct namespace
kubectl get instrumentation -n cxtm

# 2. App has the annotation
kubectl describe deployment cxtm-web -n cxtm | grep instrumentation

# 3. Pod restarted with OpenTelemetry
kubectl get pods -n cxtm | grep cxtm-web
```

### Step 2: Generate Test Traffic
```bash
# Port forward to test the app
kubectl port-forward -n cxtm service/cxtm-web 8081:8080 &

# Make some requests
curl http://localhost:8081/
curl http://localhost:8081/healthz
```

### Step 3: Check Splunk Observability
1. Go to: https://cisco-cx-observe.signalfx.com
2. Navigate: APM â†’ Services  
3. Search for: "cxtm"
4. Look for: `cxtm-web` service with activity

**What you should see**:
- Service name: `cxtm-web`
- Request counts and response times
- Connected services: `redis`, `mysql`, `mysql:tm2`
- Individual traces with timing details

---

## ğŸš€ Applying to Other Services (Easy Steps)

### For Each Additional CXTM Service:

**Step 1**: Add the annotation
```bash
kubectl annotate deployment <service-name> -n cxtm \
  instrumentation.opentelemetry.io/inject-python=cxtm-python-instrumentation --overwrite
```

**Step 2**: Set service name
```bash
kubectl set env deployment/<service-name> -n cxtm OTEL_SERVICE_NAME=<service-name>
```

**Step 3**: Wait for restart and test
```bash
kubectl get pods -n cxtm | grep <service-name>
```

### Services Ready for Instrumentation:
- `cxtm-scheduler`
- `cxtm-zipservice`  
- `cxtm-taskdriver`
- `cxtm-webcron`
- `cxtm-logstream`

---

## ğŸ¯ Key Takeaways for Beginners

### 1. Namespace Scoping is Critical
- **Rule**: Instrumentation config must be in the SAME namespace as your apps
- **Our setup**: Config in `cxtm` namespace, apps in `cxtm` namespace âœ…
- **Data flow**: Config in `cxtm` â†’ sends data to collector in `ao` âœ…

### 2. OpenTelemetry Operator Does Two Things
- **Injects libraries**: Adds OpenTelemetry code to your pods (automatic)
- **Modifies startup**: May or may not wrap your app command (sometimes manual)

### 3. Cross-Namespace Communication Works
- Apps in `cxtm` can send data to collector in `ao`
- Use full service DNS names: `service-name.namespace.svc.cluster.local`

### 4. Validation is Key
- Always check: pods restarted, annotations applied, traces generated
- Test with real traffic, not just pod status checks
- Verify end-to-end: app â†’ collector â†’ Splunk Cloud

---

## ğŸ“ Files You Need

Save these files for future use:

**1. `python-instrumentation.yaml`** - The configuration
**2. Deployment patch commands** - To fix startup commands  
**3. Validation commands** - To check if it's working
**4. This guide** - For your team to understand the process

---

## ğŸ‰ Success Criteria Checklist

- [ ] App visible in Splunk Observability Cloud
- [ ] Real traces showing (not just metrics)
- [ ] Service dependencies visible (database, cache connections)
- [ ] Response times under 10ms for healthy requests
- [ ] Zero errors in normal operation
- [ ] Team can access and understand the data

**When all checkboxes are âœ…, your POC is successful!**

---

## ğŸ” Our Debugging Techniques That Worked

### Key Diagnostic Commands We Used
```bash
# 1. Environment Discovery (Always start here!)
kubectl get namespaces
kubectl get all --all-namespaces | grep -E "(otel|splunk)"
kubectl get instrumentation --all-namespaces

# 2. Pod Investigation
kubectl get pods -n cxtm | grep cxtm-web
kubectl describe pod <pod-name> -n cxtm | grep -A10 "Init Containers"
kubectl exec <pod-name> -n cxtm -- ps aux | head -5

# 3. Instrumentation Verification  
kubectl describe deployment cxtm-web -n cxtm | grep instrumentation
kubectl exec <pod-name> -n cxtm -- env | grep OTEL
kubectl exec <pod-name> -n cxtm -- ls -la /otel-auto-instrumentation/

# 4. Traffic Testing
kubectl port-forward -n cxtm service/cxtm-web 8081:8080 &
curl http://localhost:8081/healthz
kubectl logs -l app=splunk-otel-collector -n ao --tail=50
```

### Problem-Solving Approach
1. **Start broad** â†’ Check if infrastructure exists
2. **Narrow down** â†’ Identify namespace and component issues  
3. **Test incrementally** â†’ One change at a time
4. **Validate continuously** â†’ Check after each change
5. **Document everything** â†’ What worked, what didn't

---

## â±ï¸ Timeline: How Long Each Phase Took

### Day 1 (August 19) - Learning Phase
- **Hour 1**: Environment discovery and planning
- **Hour 2-3**: Helm deployment and initial configuration 
- **Hour 4**: Troubleshooting webhook issues
- **Result**: Infrastructure deployed, application instrumentation failed

### Day 2 (August 20) - Success Phase  
- **Minutes 1-15**: Discovered existing infrastructure
- **Minutes 15-30**: Created namespace-scoped instrumentation
- **Minutes 30-60**: Fixed application startup command issues
- **Minutes 60-90**: Resolved file path and directory problems
- **Minutes 90+**: Validation and trace generation
- **Result**: 720+ traces successfully captured!

**Key Insight**: Second day was much faster because we:
- Built on existing infrastructure
- Applied lessons learned from day 1
- Used systematic debugging approach

---

## ğŸ“ Getting Help (What to Share)

**If you get stuck, share these details:**

1. **Environment Info**:
   ```bash
   kubectl version --short
   kubectl get namespaces
   kubectl get instrumentation --all-namespaces
   ```

2. **Pod Status**:
   ```bash
   kubectl get pods -n cxtm
   kubectl describe pod <failing-pod> -n cxtm
   ```

3. **Logs**:
   ```bash
   kubectl logs <pod-name> -n cxtm
   kubectl logs -l app=splunk-otel-collector -n ao --tail=20
   ```

4. **What You Expected vs What Happened**:
   - "Expected: Traces in Splunk O11y"
   - "Actual: No services showing"
   - "Error: Pod crashing with config file error"

---

## ğŸ“ Key Success Factors

### Technical Factors
1. **Namespace alignment** - instrumentation and apps in same namespace
2. **Existing infrastructure utilization** - don't reinvent the wheel
3. **Proper startup command wrapping** - OpenTelemetry must wrap the app
4. **Correct file paths** - working directory and config file locations

### Process Factors  
1. **Incremental approach** - one change at a time
2. **Systematic debugging** - follow logical troubleshooting steps
3. **Team collaboration** - work with existing infrastructure teams
4. **Documentation** - record what works for future use

### Mindset Factors
1. **Persistence** - first attempts often fail, keep iterating
2. **Learning orientation** - each failure teaches something valuable
3. **Collaboration** - leverage others' work and expertise
4. **Systematic thinking** - understand the whole system, not just parts

---

**ğŸ‰ Final Achievement: From 0 to 720+ traces in 2 days!**

*This guide captures our complete journey from August 19-20, 2025*  
*Environment: CALO Lab Kubernetes cluster (uta-k8s)*  
*Team: Working with Preeti's infrastructure monitoring setup*  
*Result: Production-ready CXTM observability in Splunk O11y Cloud*