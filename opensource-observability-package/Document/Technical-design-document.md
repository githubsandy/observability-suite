# Enhanced Observability Stack - Technical Design Document

**Version:** 3.0  
**Date:** September 2025  
**Authors:** CALO Lab Engineering Team  
**Status:** Production Deployed (CALO Lab) - 17 Services Active
**Deployment:** ao-observability (revision 25+) in ao-os namespace

---

## ğŸ“‹ Executive Summary

The Enhanced Observability Stack is a comprehensive, infrastructure-agnostic monitoring solution designed to provide complete **Logs + Metrics + Traces (L.M.T)** observability for Kubernetes environments. This document outlines the technical architecture, design decisions, and implementation details of a production-ready observability platform optimized for the University of Texas at Arlington (UTA) CALO lab environment while maintaining universal deployability.

### Key Innovations

- **Complete L.M.T Integration**: Unified logs, metrics, and distributed tracing
- **Zero External Dependencies**: Self-hosted solution with no SaaS costs
- **Auto-Discovery Intelligence**: Dynamic service detection and configuration
- **Infrastructure Agnostic**: Deploy anywhere with intelligent environment adaptation
- **Advanced Network Monitoring**: Comprehensive path analysis and latency monitoring
- **Production-Grade Alerting**: Pre-configured rules and intelligent routing

---

## ğŸ¯ Project Objectives

### Primary Goals

1. **Complete Observability Coverage**
   - Centralized logging with advanced correlation
   - Comprehensive metrics collection and analysis  
   - Distributed tracing for microservices architecture
   - Real-time alerting with intelligent noise reduction
   - **Container-level monitoring** across all cluster nodes

2. **Zero External Cost Dependency**
   - Self-contained deployment model
   - No external SaaS requirements
   - Air-gapped environment compatibility
   - Complete data sovereignty

3. **Infrastructure Intelligence**
   - Automatic environment detection and adaptation
   - Dynamic service discovery and configuration
   - Smart resource allocation and optimization
   - Multi-cloud and on-premises compatibility
   - **Direct NodePort access** without ingress complexity

4. **CALO Lab Optimization**
   - Specialized integration with CXTAF/CXTM frameworks
   - UTA network topology awareness
   - Test automation workflow monitoring
   - Research environment optimization
   - **16-node cluster coverage** with DaemonSet deployments

## ğŸ† Current Production State (Version 3.0)

### Deployment Statistics
- **Total Services**: 17 production services
- **Success Rate**: 100% deployment success
- **Cluster Coverage**: 16-node CALO lab cluster
- **Namespace**: ao-os (plug & play)
- **Access Method**: Direct NodePort (no port-forwarding)
- **Auto-configuration**: Grafana with pre-configured datasources

### Service Distribution
```
ğŸ“Š Core Observability: 4 services
â”œâ”€â”€ Prometheus (metrics database)
â”œâ”€â”€ Grafana (visualization + auto-configured datasources)
â”œâ”€â”€ Loki (log aggregation)
â””â”€â”€ Promtail (log collection)

ğŸš¨ Advanced Monitoring: 4 services  
â”œâ”€â”€ Tempo (distributed tracing)
â”œâ”€â”€ AlertManager (alert routing)
â”œâ”€â”€ Smokeping (network latency)
â””â”€â”€ MTR (network diagnostics)

ğŸ”§ Infrastructure Monitoring: 4 services
â”œâ”€â”€ Node Exporter (15 DaemonSet pods - system metrics)
â”œâ”€â”€ cAdvisor (15 DaemonSet pods - container metrics)
â”œâ”€â”€ Blackbox Exporter (endpoint monitoring)
â””â”€â”€ kube-state-metrics (K8s object metrics)

ğŸ“¦ Application/Database Monitoring: 5 services
â”œâ”€â”€ MongoDB Exporter (database performance)
â”œâ”€â”€ PostgreSQL Exporter (database metrics)
â”œâ”€â”€ Redis Exporter (cache performance)
â”œâ”€â”€ Jenkins Exporter (CI/CD pipeline metrics)
â””â”€â”€ FastAPI Metrics (test automation metrics)
```

### Production Validation
- **CALO Lab Deployment**: Fully operational since September 2025
- **Multi-Node Coverage**: DaemonSets running on all 16 cluster nodes
- **Direct Access**: All services accessible via NODE-IP:PORT
- **Performance**: Optimized resource allocation and monitoring
- **Reliability**: Persistent storage, proper RBAC, security contexts

### Service Access Configuration (NodePort)
```
Primary Services (Direct Access):
â”œâ”€â”€ Grafana Dashboard        â†’ http://10.122.28.111:30300
â”œâ”€â”€ Prometheus Server        â†’ http://10.122.28.111:30090
â”œâ”€â”€ AlertManager Console     â†’ http://10.122.28.111:30930
â”œâ”€â”€ Tempo Tracing API        â†’ http://10.122.28.111:30320
â”œâ”€â”€ Smokeping Network UI     â†’ http://10.122.28.111:30800
â”œâ”€â”€ Loki Log API             â†’ http://10.122.28.111:30310
â”œâ”€â”€ cAdvisor Container UI    â†’ http://10.122.28.111:30080
â”œâ”€â”€ Blackbox Endpoint UI     â†’ http://10.122.28.111:30115
â””â”€â”€ MTR Network Diagnostics  â†’ http://10.122.28.111:30808

Internal Services (ClusterIP):
â”œâ”€â”€ Database Exporters       â†’ Scraped by Prometheus
â”œâ”€â”€ Application Exporters    â†’ Auto-discovered targets
â””â”€â”€ DaemonSet Services       â†’ Node-local access
```

### Auto-Configuration Features
- **Grafana Datasources**: Prometheus, Loki, and Tempo pre-configured
- **Prometheus Targets**: All exporters automatically discovered
- **Service Discovery**: Kubernetes API integration for dynamic targets
- **Security Context**: Non-root containers with proper permissions
- **Resource Management**: Production-ready CPU/memory limits

---

## ğŸ—ï¸ System Architecture

### High-Level Architecture Overview

<img src="unifiedDataFlow.png" alt="Architecture" width="600" height="700">

### Component Architecture

#### Core Observability Layer

**1. Metrics Collection (Prometheus Ecosystem)**
```
Prometheus Server
â”œâ”€â”€ Enhanced Service Discovery
â”‚   â”œâ”€â”€ Kubernetes API Integration
â”‚   â”œâ”€â”€ CXTAF/CXTM Auto-Detection
â”‚   â””â”€â”€ Cross-Namespace Monitoring
â”œâ”€â”€ Storage & Retention
â”‚   â”œâ”€â”€ Local TSDB Storage (30GB+)
â”‚   â”œâ”€â”€ Configurable Retention Policies
â”‚   â””â”€â”€ High-Availability Support
â””â”€â”€ Export Integrations
    â”œâ”€â”€ Infrastructure Exporters
    â”‚   â”œâ”€â”€ Node Exporter (System metrics - DaemonSet)
    â”‚   â”œâ”€â”€ cAdvisor (Container metrics - DaemonSet on 16 nodes)
    â”‚   â”œâ”€â”€ kube-state-metrics (Kubernetes object metrics)
    â”‚   â””â”€â”€ Blackbox Exporter (Endpoint monitoring)
    â”œâ”€â”€ Database Exporters
    â”‚   â”œâ”€â”€ MongoDB Exporter (Database performance)
    â”‚   â”œâ”€â”€ PostgreSQL Exporter (Database metrics)
    â”‚   â””â”€â”€ Redis Exporter (Cache performance)
    â”œâ”€â”€ Application Exporters
    â”‚   â”œâ”€â”€ Jenkins Exporter (CI/CD metrics)
    â”‚   â””â”€â”€ FastAPI Metrics (Test automation metrics)
    â””â”€â”€ Network Monitoring
        â”œâ”€â”€ Smokeping (Latency monitoring)
        â””â”€â”€ MTR (Network diagnostics)
```

**2. Log Aggregation (Loki Ecosystem)**
```
Loki Server
â”œâ”€â”€ Log Ingestion
â”‚   â”œâ”€â”€ Promtail DaemonSet Collection
â”‚   â”œâ”€â”€ Structured Log Parsing
â”‚   â””â”€â”€ Label Extraction & Indexing
â”œâ”€â”€ Storage Architecture
â”‚   â”œâ”€â”€ Object Storage Compatible
â”‚   â”œâ”€â”€ Chunk-based Storage Model
â”‚   â””â”€â”€ Configurable Retention
â””â”€â”€ Query Engine
    â”œâ”€â”€ LogQL Advanced Queries
    â”œâ”€â”€ Trace Correlation Support
    â””â”€â”€ Metrics Extraction
```

**3. Distributed Tracing (Tempo Direct Ingestion)**
```
Tempo Backend
â”œâ”€â”€ Trace Ingestion
â”‚   â”œâ”€â”€ Multi-Protocol Support (OTLP, Jaeger, Zipkin)
â”‚   â”œâ”€â”€ Direct Application Integration
â”‚   â””â”€â”€ Direct multi-protocol ingestion
â”œâ”€â”€ Trace Storage
â”‚   â”œâ”€â”€ Block-based Storage Model
â”‚   â”œâ”€â”€ Efficient Compression
â”‚   â””â”€â”€ Scalable Architecture
â””â”€â”€ Correlation Engine
    â”œâ”€â”€ Trace-to-Logs Correlation
    â”œâ”€â”€ Trace-to-Metrics Linking
    â””â”€â”€ Service Map Generation

Direct Application Integration
â”œâ”€â”€ Multi-Protocol Ingestion (No Collector Needed)
â”‚   â”œâ”€â”€ OTLP (gRPC and HTTP)
â”‚   â”œâ”€â”€ Jaeger (gRPC and HTTP)
â”‚   â””â”€â”€ Zipkin (HTTP)
â”œâ”€â”€ Direct Application Configuration
â”‚   â”œâ”€â”€ Applications send traces directly to Tempo
â”‚   â”œâ”€â”€ Simple endpoint configuration
â”‚   â””â”€â”€ No intermediate components required
â””â”€â”€ Self-Hosted Benefits
    â”œâ”€â”€ Complete control over trace data
    â”œâ”€â”€ Zero external dependencies
    â””â”€â”€ Cost-free trace storage and analysis
```

#### Enhanced Monitoring Layer

**4. Network Intelligence**
```
Smokeping Network Monitor
â”œâ”€â”€ Multi-Target Monitoring
â”‚   â”œâ”€â”€ CALO Lab Internal Services
â”‚   â”œâ”€â”€ External Endpoint Testing
â”‚   â””â”€â”€ Network Path Visualization
â”œâ”€â”€ Performance Analytics
â”‚   â”œâ”€â”€ Latency Trend Analysis
â”‚   â”œâ”€â”€ Packet Loss Detection
â”‚   â””â”€â”€ Historical Performance Data
â””â”€â”€ Integration Points
    â”œâ”€â”€ Prometheus Metrics Export
    â”œâ”€â”€ Grafana Dashboard Integration
    â””â”€â”€ Alert Rule Integration

MTR Network Path Analyzer
â”œâ”€â”€ Real-Time Path Discovery
â”‚   â”œâ”€â”€ Hop-by-Hop Analysis
â”‚   â”œâ”€â”€ Route Optimization Insights
â”‚   â””â”€â”€ Network Topology Mapping
â”œâ”€â”€ Performance Metrics
â”‚   â”œâ”€â”€ Per-Hop Latency Measurement
â”‚   â”œâ”€â”€ Packet Loss Analysis
â”‚   â””â”€â”€ Path Stability Monitoring
â””â”€â”€ Python-Based Implementation
    â”œâ”€â”€ Custom Metrics Collection
    â”œâ”€â”€ Prometheus Integration
    â””â”€â”€ Configurable Target Management

Enhanced Blackbox Exporter
â”œâ”€â”€ Comprehensive Module Library (15+)
â”‚   â”œâ”€â”€ HTTP/HTTPS Endpoint Testing
â”‚   â”œâ”€â”€ TCP Port Connectivity
â”‚   â”œâ”€â”€ DNS Resolution Monitoring
â”‚   â”œâ”€â”€ SSL Certificate Validation
â”‚   â”œâ”€â”€ ICMP Network Testing
â”‚   â””â”€â”€ Custom CXTAF/CXTM Health Checks
â”œâ”€â”€ Advanced Configuration
â”‚   â”œâ”€â”€ Module-Based Architecture
â”‚   â”œâ”€â”€ Configurable Timeouts & Retries
â”‚   â””â”€â”€ Custom Success Criteria
â””â”€â”€ Integration Features
    â”œâ”€â”€ Prometheus Metrics Export
    â”œâ”€â”€ Grafana Dashboard Support
    â””â”€â”€ Alert Rule Templates
```

**5. Production Alerting (AlertManager)**
```
AlertManager System
â”œâ”€â”€ Alert Processing Pipeline
â”‚   â”œâ”€â”€ Rule Evaluation Engine
â”‚   â”œâ”€â”€ Alert Grouping & Deduplication
â”‚   â””â”€â”€ Silence Management
â”œâ”€â”€ Notification Routing
â”‚   â”œâ”€â”€ Multi-Channel Support (Slack, Email, Webhook)
â”‚   â”œâ”€â”€ Escalation Policies
â”‚   â””â”€â”€ CALO Lab Specific Routing
â”œâ”€â”€ Alert Rules Library
â”‚   â”œâ”€â”€ Infrastructure Health Rules
â”‚   â”œâ”€â”€ Application Performance Rules
â”‚   â”œâ”€â”€ Network Performance Rules
â”‚   â””â”€â”€ CXTAF/CXTM Specific Rules
â””â”€â”€ High Availability
    â”œâ”€â”€ Cluster Mode Support
    â”œâ”€â”€ State Synchronization
    â””â”€â”€ Failover Mechanisms
```

#### Intelligence & Automation Layer

**6. Auto-Discovery Engine**
```
Service Discovery System
â”œâ”€â”€ Kubernetes Integration
â”‚   â”œâ”€â”€ API Server Monitoring
â”‚   â”œâ”€â”€ Resource Watch Streams
â”‚   â””â”€â”€ Dynamic Configuration Updates
â”œâ”€â”€ CALO Lab Specific Detection
â”‚   â”œâ”€â”€ CXTAF Service Auto-Detection
â”‚   â”œâ”€â”€ CXTM Workflow Monitoring
â”‚   â””â”€â”€ Database Service Discovery
â”œâ”€â”€ Configuration Management
â”‚   â”œâ”€â”€ Dynamic Prometheus Targets
â”‚   â”œâ”€â”€ Grafana Datasource Updates
â”‚   â””â”€â”€ Alert Rule Provisioning
â””â”€â”€ Environment Adaptation
    â”œâ”€â”€ Cluster Capability Detection
    â”œâ”€â”€ Storage Class Selection
    â””â”€â”€ Resource Optimization
```

**7. Visualization Layer (Enhanced Grafana)**
```
Grafana Platform
â”œâ”€â”€ Auto-Configured Integration
â”‚   â”œâ”€â”€ Pre-configured Datasources
â”‚   â”‚   â”œâ”€â”€ Prometheus (Metrics)
â”‚   â”‚   â”œâ”€â”€ Loki (Logs)
â”‚   â”‚   â”œâ”€â”€ Tempo (Traces)
â”‚   â”‚   â””â”€â”€ AlertManager (Alerts)
â”‚   â””â”€â”€ Intelligent Correlation
â”‚       â”œâ”€â”€ Trace-to-Log Linking
â”‚       â”œâ”€â”€ Metric Exemplars
â”‚       â””â”€â”€ Cross-Service Navigation
â”œâ”€â”€ Dashboard Library
â”‚   â”œâ”€â”€ Infrastructure Overview
â”‚   â”œâ”€â”€ Application Performance
â”‚   â”œâ”€â”€ Network Analysis
â”‚   â”œâ”€â”€ CXTAF/CXTM Monitoring
â”‚   â””â”€â”€ Alert Management
â””â”€â”€ Advanced Features
    â”œâ”€â”€ Custom Panel Development
    â”œâ”€â”€ Plugin Management
    â””â”€â”€ User Management Integration
```

### Production Component Inventory

The CALO Lab deployment consists of **14 active components** providing comprehensive observability coverage:

#### ğŸ“Š **Core Observability Stack (4 Components)**
- **Prometheus** - Metrics collection and monitoring
- **Grafana** - Visualization and dashboards  
- **Loki** - Log aggregation and querying
- **Promtail** - Log collection agent

#### ğŸ”§ **Infrastructure Exporters (2 Components)**
- **Node Exporter** - System metrics (CPU, Memory, Disk, Network)
- **Blackbox Exporter** - External endpoint monitoring and health checks

#### âš¡ **Foundation Exporters (3 Components)**  
- **kube-state-metrics** - Kubernetes cluster state metrics
- **MongoDB Exporter** - NoSQL database performance metrics
- **PostgreSQL Exporter** - Relational database metrics
- **Redis Exporter** - Cache and session store metrics

#### ğŸš€ **Enhanced Monitoring (4 Components)**
- **Tempo** - Distributed tracing and performance analysis
- **Alertmanager** - Alert routing and notification management
- **Smokeping** - Network latency and connectivity monitoring
- **MTR Analyzer** - Network path analysis and diagnostics

#### âŒ **Disabled Components (2 Components)**
- **Jenkins Exporter** - Disabled due to deprecated Schema 1 image format
- **FastAPI Metrics** - Disabled for current deployment (not required)

**Total Active Components:** 14 out of 16 available components providing production-ready observability.

### Production Component Status Matrix

#### âœ… Core Observability Stack

| Component | Status | Resource Usage | Description |
|-----------|--------|----------------|-------------|
| **Prometheus** | âœ… Running | 250m CPU, 256Mi RAM | Time-series metrics database |
| **Grafana** | âœ… Running | 250m CPU, 256Mi RAM | Visualization and alerting UI |
| **Loki** | âœ… Running | 250m CPU, 256Mi RAM | Log aggregation system |
| **Promtail** | âœ… Running | 100m CPU, 64Mi RAM | Log collection agent |

#### âœ… Infrastructure Monitoring

| Component | Status | Deployment Type | Metrics Provided |
|-----------|--------|-----------------|------------------|
| **Node Exporter** | âœ… Running | DaemonSet | CPU, Memory, Disk, Network |
| **Blackbox Exporter** | âœ… Running | Deployment | HTTP, DNS, TCP endpoint health |
| **kube-state-metrics** | âœ… Running | Deployment | Pod, Service, Deployment status |

#### âœ… Enhanced Capabilities

| Component | Status | Features | Use Case |
|-----------|--------|----------|----------|
| **Tempo** | âœ… Running | Distributed tracing | Request flow analysis |
| **Alertmanager** | âœ… Running | Alert routing | Incident management |
| **Smokeping** | âœ… Running | Network latency graphs | Network performance |
| **MTR Analyzer** | âœ… Running | Network path analysis | Network troubleshooting |

#### âœ… Database & Application Monitoring

| Component | Status | Target Systems | Metrics |
|-----------|--------|----------------|---------|
| **MongoDB Exporter** | âœ… Running | NoSQL databases | Connections, operations, performance |
| **PostgreSQL Exporter** | âœ… Running | SQL databases | Queries, connections, replication |
| **Redis Exporter** | âœ… Running | Cache systems | Memory usage, commands, clients |

#### âŒ Disabled Components

| Component | Status | Reason | Alternative |
|-----------|--------|--------|-------------|
| **Jenkins Exporter** | âŒ Disabled | Deprecated Schema 1 image format | Use Prometheus Jenkins plugin |
| **FastAPI Metrics** | âŒ Disabled | Not required for current deployment | Enable when custom app metrics needed |

### Monitoring Capabilities & Queries

#### Prometheus Metrics Collection Examples

```yaml
# System Health Monitoring
up                                       # Service availability
node_cpu_seconds_total                   # CPU usage
node_memory_MemAvailable_bytes          # Available memory

# Kubernetes Health Monitoring
kube_pod_status_phase                    # Pod lifecycle states
kube_deployment_status_replicas          # Deployment health
kube_node_status_condition               # Node conditions

# Application Performance Monitoring
fastapi_request_duration_seconds         # API response times
test_executions_total                    # Test automation metrics
```

#### Loki Log Queries

```logql
# Log Filtering Examples
{namespace="ao-os"}                      # Observability namespace logs
{app="prometheus"} |= "error"           # Error logs from Prometheus
{job="varlogs"} | json | level="ERROR"  # Structured error logs
```

#### Tempo Tracing Configuration

```bash
# Trace Collection Endpoints
# Jaeger: http://tempo:14268/api/traces
# Zipkin: http://tempo:9411/api/v2/spans  
# OTLP gRPC: tempo:4317
# OTLP HTTP: tempo:4318
```

---

## ğŸ”§ Technical Implementation Details

### Deployment Architecture

#### Helm Chart Structure
```
helm-kube-observability-stack/
â”œâ”€â”€ Chart.yaml                     # Chart metadata
â”œâ”€â”€ values.yaml                    # Dynamic configuration
â””â”€â”€ templates/
    â”œâ”€â”€ core/                      # Core observability components
    â”‚   â”œâ”€â”€ prometheus/
    â”‚   â”œâ”€â”€ grafana/
    â”‚   â””â”€â”€ loki/
    â”œâ”€â”€ enhanced/                  # Enhanced monitoring services
    â”‚   â”œâ”€â”€ tempo/
    â”‚   â”œâ”€â”€ tempo/                        # Direct tracing ingestion
    â”‚   â””â”€â”€ alertmanager/
    â”œâ”€â”€ network/                   # Network monitoring suite
    â”‚   â”œâ”€â”€ smokeping/
    â”‚   â”œâ”€â”€ mtr/
    â”‚   â””â”€â”€ blackbox/
    â”œâ”€â”€ infrastructure/            # Infrastructure exporters
    â”‚   â”œâ”€â”€ node-exporter/
    â”‚   â”œâ”€â”€ kube-state-metrics/
    â”‚   â”œâ”€â”€ cadvisor/              # Container-level metrics
    â”‚   â””â”€â”€ promtail/
    â””â”€â”€ automation/               # Auto-discovery & configuration
        â”œâ”€â”€ service-discovery/
        â””â”€â”€ dynamic-config/
```

#### Configuration Management

**Dynamic Values Architecture:**
```yaml
# Environment-specific configuration
environment:
  name: "calo_lab"              # YAML-compatible underscore format (not "calo-lab")
  namespace: "ao-os"            # Actual CALO lab deployment namespace
  
  cluster:
    detection:
      enabled: true             # Enable auto-detection
      nodeSelector:
        strategy: "labels"      # Node selection strategy
        labels:
          ao-node: observability
    
    storage:
      autoDetect: true          # Auto-detect storage classes
      preferredClass: "longhorn-single"
      
    network:
      topology: "calo-lab"      # Network topology awareness
      
# Component enablement matrix
components:
  core:
    prometheus: true
    grafana: true
    loki: true
    promtail: true
    
  enhanced:
    tempo: true                 # Distributed tracing
    alertmanager: true          # Production alerting
    cadvisor: true              # Container-level metrics via kubelet
    # Direct Tempo ingestion - no collector dependencies
    
  network:
    smokeping: true            # Network latency monitoring
    mtrAnalyzer: true          # Path analysis
    blackboxEnhanced: true     # Comprehensive endpoint testing
    
  applications:
    jenkins: false             # Disabled due to deprecated image format
    fastapi: false             # Disabled for current deployment
    
  databases:
    mongodb: true              # MongoDB exporter active
    postgresql: true           # PostgreSQL exporter active  
    redis: true                # Redis exporter active
    
  intelligence:
    autoDiscovery: true        # Service auto-discovery
    cxtafIntegration: true     # CXTAF framework integration
    cxtmIntegration: true      # CXTM framework integration
    
# Performance optimization
performance:
  sizing:
    environment: "medium"      # small, medium, large
    
  retention:
    metrics: "30d"             # Prometheus retention
    logs: "168h"               # Loki retention (Go duration format)
    traces: "168h"             # Tempo retention (Go duration format - was "7d")
    
  scrapeIntervals:
    default: "30s"
    infrastructure: "15s"
    network: "60s"
```

#### Dynamic Namespace Resolution

**Template Logic for Infrastructure Portability:**
```yaml
# All Helm templates use dynamic namespace resolution
namespace: {{ .Values.environment.namespace | default .Values.namespace }}

# Resolution Priority:
# 1. environment.namespace (e.g., "ao-os" for CALO lab)
# 2. namespace (fallback: "kube-observability-stack")
```

**Benefits:**
- **Environment Consistency**: All 14 active components deploy to the same namespace
- **Multi-Environment Support**: Different namespaces per deployment target
- **Backward Compatibility**: Maintains fallback for legacy configurations  
- **Infrastructure Agnostic**: Works across cloud providers and on-premises
- **Production Tested**: Configuration validated through multiple deployment cycles

### Data Flow Architecture

#### Metrics Pipeline
```
Application/Infrastructure
         â†“
[Exporters] â†’ [Prometheus] â†’ [Grafana Visualization]
         â†“                        â†‘
[Service Discovery] â†â†’ [AlertManager] â†â†’ [Alert Rules]
```

#### Logs Pipeline  
```
Applications/Containers
         â†“
[Promtail Collection] â†’ [Loki Ingestion] â†’ [LogQL Processing]
         â†“                        â†“
[Label Extraction] â†’ [Index Creation] â†’ [Grafana Visualization]
```

#### Traces Pipeline
```
Applications (Auto-Instrumented)
         â†“
[Applications] â†’ [Direct Tempo Ingestion] â†’ [Trace Storage]
         â†“                        â†“
[Sampling & Processing] â†’ [Correlation Engine] â†’ [Grafana Visualization]
```

#### Network Monitoring Pipeline
```
Network Targets
         â†“
[Smokeping] â†â†’ [MTR Analyzer] â†â†’ [Blackbox Exporter]
         â†“              â†“              â†“
[Prometheus Metrics] â†’ [Grafana Dashboards] â†’ [Alert Rules]
```

### CALO Lab Production Access Configuration

#### NodePort Service Access (Production)

The CALO lab deployment uses NodePort services for direct IP access without ingress complexity:

| Service | NodePort | Internal Port | URL Pattern | Description |
|---------|----------|---------------|-------------|-------------|
| **Grafana** | 30300 | 3000 | `http://NODE-IP:30300` | Primary observability dashboard |
| **Prometheus** | 30090 | 9090 | `http://NODE-IP:30090` | Metrics collection and querying |
| **Loki** | 30310 | 3100 | `http://NODE-IP:30310` | Log aggregation and search |
| **Alertmanager** | 30930 | 9093 | `http://NODE-IP:30930` | Alert management and routing |
| **Tempo** | 30320 | 3200 | `http://NODE-IP:30320` | Distributed tracing interface |
| **Blackbox** | 30115 | 9115 | `http://NODE-IP:30115` | Endpoint health monitoring |

#### Production Environment Status

```yaml
# Current CALO Lab Deployment
release_name: ao-observability
namespace: ao-os
revision: 16
status: deployed
chart_version: kube-observability-stack-1.0.0
active_components: 14
deployment_path: /home/administrator/skumark5/osp/observability-suite/opensource-observability-package
```

#### Service Discovery Commands

```bash
# Get cluster node IPs for access
kubectl get nodes -o wide

# Verify all services are exposed
kubectl get svc -n ao-os | grep NodePort

# Check specific service health
curl -I http://YOUR-NODE-IP:30090/-/healthy   # Prometheus
curl -I http://YOUR-NODE-IP:30310/ready       # Loki  
curl -I http://YOUR-NODE-IP:30300/api/health  # Grafana
```

---

## ğŸ¯ Design Decisions & Rationale

### Architecture Decisions

#### 1. Microservices vs Monolithic Approach
**Decision:** Microservices architecture with containerized components  
**Rationale:** 
- Independent scaling and lifecycle management
- Technology diversity support (Go, Python, etc.)
- Fault isolation and resilience
- Kubernetes-native deployment patterns

#### 2. Pull vs Push Model for Metrics
**Decision:** Hybrid approach with Prometheus pull model + Direct Tempo ingestion  
**Rationale:**
- Prometheus pull model for infrastructure metrics (reliable, efficient)
- Direct Tempo ingestion for traces (simple, no dependencies)
- Self-hosted trace analysis (cost-free)
- Service discovery integration for dynamic environments

#### 3. Storage Strategy
**Decision:** Local storage with cloud-compatible architecture  
**Rationale:**
- Zero external dependencies requirement
- Cost optimization (no cloud storage fees)
- Data sovereignty and security compliance
- Migration path to cloud storage if needed

#### 4. Auto-Discovery Implementation
**Decision:** Kubernetes-native service discovery with custom CALO lab extensions  
**Rationale:**
- Dynamic environment adaptation
- Reduced manual configuration overhead
- Framework-specific optimizations (CXTAF/CXTM)
- Scalable to large cluster deployments

### Technology Choices

#### Core Technologies

| Component | Technology | Rationale |
|-----------|------------|-----------|
| **Metrics** | Prometheus | Industry standard, excellent Kubernetes integration, powerful query language |
| **Logs** | Loki | Lightweight, Prometheus-like architecture, excellent Grafana integration |
| **Traces** | Tempo Direct Ingestion | Modern tracing stack, multi-protocol support, excellent correlation capabilities |
| **Visualization** | Grafana | Best-in-class visualization, extensive datasource support, rich ecosystem |
| **Alerting** | AlertManager | Native Prometheus integration, sophisticated routing, production-proven |

#### Enhanced Technologies

| Component | Technology | Rationale |
|-----------|------------|-----------|
| **Container Metrics** | cAdvisor via kubelet | Zero-pod container resource monitoring, CPU/memory/network per container, built-in Kubernetes integration |
| **Network Monitoring** | Smokeping + MTR | Proven network analysis tools, rich visualization, historical data |
| **Endpoint Testing** | Enhanced Blackbox | Comprehensive testing modules, flexible configuration, Prometheus integration |
| **Direct Ingestion** | Tempo Multi-Protocol | Simple configuration, no dependencies, cost-free |
| **Orchestration** | Helm | Kubernetes package management standard, templating capabilities, lifecycle management |

### Security Considerations

#### Authentication & Authorization
```
Security Layer Architecture:
â”œâ”€â”€ Kubernetes RBAC Integration
â”‚   â”œâ”€â”€ Service Account Management
â”‚   â”œâ”€â”€ Role-Based Access Control
â”‚   â””â”€â”€ Network Policies
â”œâ”€â”€ Grafana Authentication
â”‚   â”œâ”€â”€ Local User Management
â”‚   â”œâ”€â”€ LDAP/OAuth Integration Support
â”‚   â””â”€â”€ Role-Based Dashboards
â””â”€â”€ Network Security
    â”œâ”€â”€ TLS Encryption (Internal Communication)
    â”œâ”€â”€ Network Segmentation
    â””â”€â”€ Ingress Security Headers
```

#### Data Security
- **Encryption at Rest:** Kubernetes secret management for sensitive configurations
- **Encryption in Transit:** TLS for all inter-service communication
- **Access Control:** Kubernetes RBAC for all components
- **Network Security:** Network policies for traffic isolation

---

## ğŸ“Š Performance & Scalability

### Performance Characteristics

#### Throughput Specifications
| Component | Metric | Specification |
|-----------|---------|---------------|
| **Prometheus** | Metrics/sec | 1M+ samples/sec |
| **Loki** | Log entries/sec | 100K+ entries/sec |
| **Tempo** | Traces/sec | 10K+ spans/sec |
| **Grafana** | Concurrent users | 100+ users |

#### Resource Requirements

**Minimum Requirements (Small Environment):**
```yaml
resources:
  small:
    prometheus:
      memory: "2Gi"
      cpu: "1000m"
    grafana:
      memory: "512Mi"
      cpu: "500m"
    loki:
      memory: "1Gi"
      cpu: "500m"
    tempo:
      memory: "1Gi"
      cpu: "500m"
```

**Recommended Requirements (Medium Environment - CALO Lab):**
```yaml
resources:
  medium:
    prometheus:
      memory: "4Gi"
      cpu: "2000m"
    grafana:
      memory: "1Gi"
      cpu: "1000m"
    loki:
      memory: "2Gi"
      cpu: "1000m"
    tempo:
      memory: "2Gi"
      cpu: "1000m"
```

**Production Requirements (Large Environment):**
```yaml
resources:
  large:
    prometheus:
      memory: "8Gi"
      cpu: "4000m"
    grafana:
      memory: "2Gi"
      cpu: "2000m"
    loki:
      memory: "4Gi"
      cpu: "2000m"
    tempo:
      memory: "4Gi"
      cpu: "2000m"
```

### Scalability Architecture

#### Horizontal Scaling Strategy
```
Component Scaling Approach:
â”œâ”€â”€ Prometheus
â”‚   â”œâ”€â”€ Federation for Multi-Cluster
â”‚   â”œâ”€â”€ Sharding by Service/Namespace
â”‚   â””â”€â”€ Remote Storage Integration
â”œâ”€â”€ Loki
â”‚   â”œâ”€â”€ Multi-Tenant Architecture
â”‚   â”œâ”€â”€ Distributed Query Engine
â”‚   â””â”€â”€ Object Storage Scaling
â”œâ”€â”€ Tempo
â”‚   â”œâ”€â”€ Microservices Mode
â”‚   â”œâ”€â”€ Distributed Ingestion
â”‚   â””â”€â”€ Scalable Query Frontend
â””â”€â”€ Grafana
    â”œâ”€â”€ Database Backend Scaling
    â”œâ”€â”€ Load Balancer Integration
    â””â”€â”€ Caching Layer Optimization
```

#### Auto-Scaling Configuration
```yaml
autoscaling:
  enabled: true
  
  prometheus:
    minReplicas: 1
    maxReplicas: 3
    targetCPU: 70
    targetMemory: 80
    
  grafana:
    minReplicas: 2
    maxReplicas: 5
    targetCPU: 60
    
  loki:
    minReplicas: 1
    maxReplicas: 3
    targetMemory: 75
```

---

## ğŸ”„ Integration Points

### CALO Lab Specific Integrations

#### CXTAF (Test Automation Framework) Integration
```
CXTAF Integration Architecture:
â”œâ”€â”€ Auto-Discovery Mechanisms
â”‚   â”œâ”€â”€ Kubernetes Service Detection
â”‚   â”œâ”€â”€ Custom Annotation Scanning
â”‚   â””â”€â”€ Dynamic Target Registration
â”œâ”€â”€ Metrics Collection
â”‚   â”œâ”€â”€ Test Execution Metrics
â”‚   â”œâ”€â”€ Device Connection Monitoring
â”‚   â”œâ”€â”€ Performance Benchmarking
â”‚   â””â”€â”€ Resource Utilization Tracking
â”œâ”€â”€ Distributed Tracing
â”‚   â”œâ”€â”€ Test Workflow Tracing
â”‚   â”œâ”€â”€ API Call Correlation
â”‚   â””â”€â”€ Performance Analysis
â””â”€â”€ Alert Integration
    â”œâ”€â”€ Test Failure Notifications
    â”œâ”€â”€ Performance Degradation Alerts
    â””â”€â”€ Resource Exhaustion Warnings
```

#### CXTM (Workflow Management) Integration
```
CXTM Integration Architecture:
â”œâ”€â”€ Database Monitoring
â”‚   â”œâ”€â”€ MariaDB Performance Metrics
â”‚   â”œâ”€â”€ Redis Cache Monitoring
â”‚   â””â”€â”€ Connection Pool Analysis
â”œâ”€â”€ Workflow Tracking
â”‚   â”œâ”€â”€ Active Workflow Monitoring
â”‚   â”œâ”€â”€ Completion Rate Analysis
â”‚   â””â”€â”€ Error Rate Tracking
â”œâ”€â”€ Resource Monitoring
â”‚   â”œâ”€â”€ CPU/Memory Utilization
â”‚   â”œâ”€â”€ Network I/O Analysis
â”‚   â””â”€â”€ Storage Performance
â””â”€â”€ Business Metrics
    â”œâ”€â”€ Workflow Success Rates
    â”œâ”€â”€ Processing Time Analysis
    â””â”€â”€ Queue Depth Monitoring
```

### External System Integration

#### University Network Integration
```
Network Integration Points:
â”œâ”€â”€ UTA Network Monitoring
â”‚   â”œâ”€â”€ Campus Network Connectivity
â”‚   â”œâ”€â”€ Internet Gateway Performance
â”‚   â””â”€â”€ Internal Service Reachability
â”œâ”€â”€ DNS Integration
â”‚   â”œâ”€â”€ Internal DNS Resolution
â”‚   â”œâ”€â”€ External DNS Performance
â”‚   â””â”€â”€ DNS Failure Detection
â””â”€â”€ Security Integration
    â”œâ”€â”€ Firewall Rule Monitoring
    â”œâ”€â”€ VPN Connectivity Tracking
    â””â”€â”€ Network Security Alerts
```

#### Multi-Cloud Integration Support
```
Cloud Integration Architecture:
â”œâ”€â”€ AWS Integration
â”‚   â”œâ”€â”€ EKS Cluster Support
â”‚   â”œâ”€â”€ CloudWatch Metrics Bridge
â”‚   â””â”€â”€ S3 Storage Backend
â”œâ”€â”€ GCP Integration
â”‚   â”œâ”€â”€ GKE Cluster Support
â”‚   â”œâ”€â”€ Cloud Monitoring Integration
â”‚   â””â”€â”€ Cloud Storage Backend
â”œâ”€â”€ Azure Integration
â”‚   â”œâ”€â”€ AKS Cluster Support
â”‚   â”œâ”€â”€ Azure Monitor Bridge
â”‚   â””â”€â”€ Blob Storage Backend
â””â”€â”€ On-Premises Integration
    â”œâ”€â”€ VMware vSphere Support
    â”œâ”€â”€ OpenStack Integration
    â””â”€â”€ Bare Metal Kubernetes
```

---

## ğŸ› ï¸ Automation & DevOps

### Deployment Automation

#### Infrastructure as Code
```
Deployment Pipeline:
â”œâ”€â”€ Environment Detection
â”‚   â”œâ”€â”€ Cluster Capability Analysis
â”‚   â”œâ”€â”€ Storage Class Discovery
â”‚   â””â”€â”€ Network Topology Assessment
â”œâ”€â”€ Dynamic Configuration
â”‚   â”œâ”€â”€ Values.yaml Generation
â”‚   â”œâ”€â”€ Resource Sizing Calculation
â”‚   â””â”€â”€ Security Policy Application
â”œâ”€â”€ Helm Deployment
â”‚   â”œâ”€â”€ Chart Validation
â”‚   â”œâ”€â”€ Progressive Rollout
â”‚   â””â”€â”€ Health Verification
â””â”€â”€ Post-Deployment Validation
    â”œâ”€â”€ Service Health Checks
    â”œâ”€â”€ Integration Testing
    â””â”€â”€ Performance Validation
```

#### CI/CD Integration
```yaml
# Example GitLab CI Pipeline
stages:
  - validate
  - test
  - deploy
  - verify

validate-chart:
  stage: validate
  script:
    - helm lint ./helm-kube-observability-stack
    - yamllint ./helm-kube-observability-stack/values.yaml

test-deployment:
  stage: test
  script:
    - ./install-observability-stack.sh test-stack test-ns test-env
    - ./verify-installation.sh test-stack test-ns
    - ./check-services.sh test-ns

deploy-production:
  stage: deploy
  script:
    - ./install-observability-stack.sh ao-observability ao-os calo_lab
  only:
    - main

verify-production:
  stage: verify
  script:
    - ./verify-installation.sh ao-observability ao-os
    - ./check-services.sh ao-os
```

### Operational Procedures

#### Backup & Recovery
```
Backup Strategy:
â”œâ”€â”€ Configuration Backup
â”‚   â”œâ”€â”€ Helm Values Export
â”‚   â”œâ”€â”€ Kubernetes Manifests
â”‚   â””â”€â”€ Custom Configurations
â”œâ”€â”€ Data Backup
â”‚   â”œâ”€â”€ Prometheus TSDB Snapshots
â”‚   â”œâ”€â”€ Grafana Dashboard Export
â”‚   â””â”€â”€ Alert Rule Backup
â”œâ”€â”€ Recovery Procedures
â”‚   â”œâ”€â”€ Disaster Recovery Playbook
â”‚   â”œâ”€â”€ Point-in-Time Recovery
â”‚   â””â”€â”€ Cross-Environment Migration
â””â”€â”€ Testing & Validation
    â”œâ”€â”€ Recovery Testing Schedule
    â”œâ”€â”€ Backup Integrity Validation
    â””â”€â”€ RTO/RPO Measurement
```

#### Monitoring & Maintenance
```
Operational Monitoring:
â”œâ”€â”€ System Health Monitoring
â”‚   â”œâ”€â”€ Component Availability
â”‚   â”œâ”€â”€ Resource Utilization
â”‚   â””â”€â”€ Performance Metrics
â”œâ”€â”€ Capacity Planning
â”‚   â”œâ”€â”€ Growth Trend Analysis
â”‚   â”œâ”€â”€ Resource Forecasting
â”‚   â””â”€â”€ Scaling Recommendations
â”œâ”€â”€ Maintenance Procedures
â”‚   â”œâ”€â”€ Update Management
â”‚   â”œâ”€â”€ Security Patching
â”‚   â””â”€â”€ Configuration Drift Detection
â””â”€â”€ Documentation
    â”œâ”€â”€ Operational Runbooks
    â”œâ”€â”€ Troubleshooting Guides
    â””â”€â”€ Knowledge Base Updates
```

---

## ğŸ” Testing & Quality Assurance

### Testing Strategy

#### Unit Testing
```
Component Testing Approach:
â”œâ”€â”€ Helm Chart Testing
â”‚   â”œâ”€â”€ Template Validation
â”‚   â”œâ”€â”€ Value Override Testing
â”‚   â””â”€â”€ Resource Generation Verification
â”œâ”€â”€ Configuration Testing
â”‚   â”œâ”€â”€ Prometheus Rule Validation
â”‚   â”œâ”€â”€ Grafana Dashboard Testing
â”‚   â””â”€â”€ Alert Rule Verification
â””â”€â”€ Script Testing
    â”œâ”€â”€ Shell Script Unit Tests
    â”œâ”€â”€ Error Condition Testing
    â””â”€â”€ Edge Case Validation
```

#### Integration Testing
```
Integration Test Suite:
â”œâ”€â”€ Service-to-Service Communication
â”‚   â”œâ”€â”€ Prometheus â† Exporters
â”‚   â”œâ”€â”€ Grafana â† Datasources
â”‚   â”œâ”€â”€ Tempo â† Direct Ingestion
â”‚   â””â”€â”€ AlertManager â† Prometheus
â”œâ”€â”€ Data Flow Validation
â”‚   â”œâ”€â”€ Metrics Collection Pipeline
â”‚   â”œâ”€â”€ Log Ingestion Pipeline
â”‚   â”œâ”€â”€ Trace Collection Pipeline
â”‚   â””â”€â”€ Alert Delivery Pipeline
â”œâ”€â”€ Auto-Discovery Testing
â”‚   â”œâ”€â”€ Service Detection Validation
â”‚   â”œâ”€â”€ Dynamic Configuration Updates
â”‚   â””â”€â”€ CXTAF/CXTM Integration
â””â”€â”€ Performance Testing
    â”œâ”€â”€ Load Testing Scenarios
    â”œâ”€â”€ Stress Testing Validation
    â””â”€â”€ Scalability Testing
```

#### Environment Testing
```
Multi-Environment Validation:
â”œâ”€â”€ CALO Lab Environment
â”‚   â”œâ”€â”€ Node Selector Validation
â”‚   â”œâ”€â”€ Storage Class Integration
â”‚   â”œâ”€â”€ Network Connectivity
â”‚   â””â”€â”€ CXTAF/CXTM Discovery
â”œâ”€â”€ Cloud Environment Testing
â”‚   â”œâ”€â”€ AWS EKS Validation
â”‚   â”œâ”€â”€ GCP GKE Testing
â”‚   â”œâ”€â”€ Azure AKS Integration
â”‚   â””â”€â”€ Multi-Cloud Compatibility
â”œâ”€â”€ On-Premises Testing
â”‚   â”œâ”€â”€ VMware Integration
â”‚   â”œâ”€â”€ OpenStack Compatibility
â”‚   â””â”€â”€ Bare Metal Kubernetes
â””â”€â”€ Edge Case Scenarios
    â”œâ”€â”€ Resource Constrained Environments
    â”œâ”€â”€ Network Partition Scenarios
    â””â”€â”€ Storage Failure Recovery
```

### Quality Metrics

#### Code Quality Standards
```yaml
quality_metrics:
  helm_charts:
    template_coverage: ">95%"
    value_validation: "100%"
    documentation: "Complete"
    
  scripts:
    error_handling: "Comprehensive"
    input_validation: "Complete"
    logging: "Structured"
    
  documentation:
    api_coverage: "100%"
    examples: "Complete"
    troubleshooting: "Comprehensive"
```

---

## ğŸ”§ Troubleshooting & Diagnostics

### CALO Lab Deployment Experience (Production Issues Resolved)

#### Critical Configuration Issues Encountered

**Revision 16 Deployment History:** The CALO lab deployment reached revision 16 through systematic resolution of multiple configuration issues:

```
CALO Lab Production Issues & Resolutions:
â”œâ”€â”€ YAML Configuration Errors
â”‚   â”œâ”€â”€ Alertmanager Route Structure
â”‚   â”‚   â”œâ”€â”€ Problem: "yaml: line 30: did not find expected key"
â”‚   â”‚   â””â”€â”€ Solution: Fixed route indentation from malformed to 10-space alignment
â”‚   â”œâ”€â”€ Environment Name Format
â”‚   â”‚   â”œâ”€â”€ Problem: "calo-lab" causing YAML key errors  
â”‚   â”‚   â””â”€â”€ Solution: Changed to "calo_lab" (underscore format)
â”‚   â””â”€â”€ Duration Format Incompatibility
â”‚       â”œâ”€â”€ Problem: "7d" format causing tempo parsing errors
â”‚       â””â”€â”€ Solution: Changed to Go duration format "168h"
â”œâ”€â”€ Container Image Issues
â”‚   â”œâ”€â”€ Jenkins Exporter Deprecation
â”‚   â”‚   â”œâ”€â”€ Problem: "Pulling Schema 1 images have been deprecated"
â”‚   â”‚   â””â”€â”€ Solution: Disabled component (components.applications.jenkins: false)
â”‚   â”œâ”€â”€ Nginx Invalid Version
â”‚   â”‚   â”œâ”€â”€ Problem: nginx:2.0.0 image not found
â”‚   â”‚   â””â”€â”€ Solution: Updated to nginx:1.24 (valid version)
â”‚   â””â”€â”€ FastAPI Metrics Configuration
â”‚       â”œâ”€â”€ Problem: Deployment startup failures
â”‚       â””â”€â”€ Solution: Disabled for current deployment
â”œâ”€â”€ Volume Permission Issues  
â”‚   â”œâ”€â”€ Grafana PVC Write Permissions
â”‚   â”‚   â”œâ”€â”€ Problem: "GF_PATHS_DATA='/var/lib/grafana' is not writable"
â”‚   â”‚   â””â”€â”€ Solution: Added initContainer with proper securityContext (user 472)
â”‚   â””â”€â”€ PVC Mounting Failures
â”‚       â”œâ”€â”€ Problem: Permission denied on data directories
â”‚       â””â”€â”€ Solution: fsGroup and initContainer configuration
â”œâ”€â”€ Template Expression Failures
â”‚   â”œâ”€â”€ Complex Resource Templates
â”‚   â”‚   â”œâ”€â”€ Problem: "unknown field spec.template.spec.containers[0].resources.sizing"
â”‚   â”‚   â””â”€â”€ Solution: Simplified to direct value references
â”‚   â””â”€â”€ Boolean Template Issues
â”‚       â”œâ”€â”€ Problem: Template parsing errors in conditional blocks
â”‚       â””â”€â”€ Solution: Fixed boolean expressions and conditional logic
â””â”€â”€ Namespace Template Conflicts
    â”œâ”€â”€ Duplicate Namespace Creation
    â”‚   â”œâ”€â”€ Problem: Helm --create-namespace conflicts with template
    â”‚   â””â”€â”€ Solution: Remove ./templates/000_namespace.yaml file
```

#### Production-Tested Solutions

**Deployment Commands That Work:**
```bash
# Remove conflicting namespace template
rm -f ./helm-kube-observability-stack/templates/000_namespace.yaml

# Production deployment
helm install ao-observability ./helm-kube-observability-stack --namespace ao-os --create-namespace

# Configuration upgrades  
helm upgrade ao-observability ./helm-kube-observability-stack --namespace ao-os
```

**Critical Values.yaml Fixes:**
```yaml
# Fixed configuration values
environment:
  name: "calo_lab"  # Underscore format (not "calo-lab")

performance:
  retention:
    loki: "168h"    # Go duration format (not "7d")
    tempo: "168h"   # Go duration format (not "7d")

components:
  applications:
    jenkins: false  # Disabled problematic component
    fastapi: false  # Disabled for current deployment

image:
  tag: "1.24"      # Valid nginx version (not "2.0.0")
```

### Common Issues & Resolutions

#### Deployment Issues
```
Issue Resolution Matrix:
â”œâ”€â”€ Helm Chart Deployment Failures
â”‚   â”œâ”€â”€ Template Rendering Errors
â”‚   â”‚   â””â”€â”€ Solution: Validate values.yaml syntax
â”‚   â”œâ”€â”€ Resource Conflicts
â”‚   â”‚   â””â”€â”€ Solution: Namespace isolation verification
â”‚   â””â”€â”€ RBAC Permission Issues
â”‚       â””â”€â”€ Solution: Service account validation
â”œâ”€â”€ Pod Startup Failures
â”‚   â”œâ”€â”€ Image Pull Errors
â”‚   â”‚   â””â”€â”€ Solution: Registry access verification
â”‚   â”œâ”€â”€ Resource Constraints
â”‚   â”‚   â””â”€â”€ Solution: Resource limit adjustment
â”‚   â””â”€â”€ Configuration Errors
â”‚       â””â”€â”€ Solution: ConfigMap validation
â””â”€â”€ Service Discovery Issues
    â”œâ”€â”€ Network Connectivity
    â”‚   â””â”€â”€ Solution: DNS and network policy check
    â”œâ”€â”€ Service Registration
    â”‚   â””â”€â”€ Solution: Kubernetes API access validation
    â””â”€â”€ Auto-Discovery Failures
        â””â”€â”€ Solution: Annotation and label verification
```

#### Performance Issues
```
Performance Troubleshooting:
â”œâ”€â”€ High Memory Usage
â”‚   â”œâ”€â”€ Prometheus Memory Issues
â”‚   â”‚   â””â”€â”€ Solution: Retention policy adjustment
â”‚   â”œâ”€â”€ Grafana Memory Leaks
â”‚   â”‚   â””â”€â”€ Solution: Dashboard optimization
â”‚   â””â”€â”€ Loki Memory Pressure
â”‚       â””â”€â”€ Solution: Chunk size optimization
â”œâ”€â”€ High CPU Usage
â”‚   â”œâ”€â”€ Query Performance Issues
â”‚   â”‚   â””â”€â”€ Solution: Query optimization
â”‚   â”œâ”€â”€ Ingestion Bottlenecks
â”‚   â”‚   â””â”€â”€ Solution: Batch size tuning
â”‚   â””â”€â”€ Processing Delays
â”‚       â””â”€â”€ Solution: Pipeline optimization
â””â”€â”€ Storage Issues
    â”œâ”€â”€ Disk Space Exhaustion
    â”‚   â””â”€â”€ Solution: Retention policy adjustment
    â”œâ”€â”€ I/O Performance
    â”‚   â””â”€â”€ Solution: Storage class optimization
    â””â”€â”€ Volume Mounting Issues
        â””â”€â”€ Solution: PV/PVC validation
```

### Diagnostic Tools

#### Built-in Diagnostics
```bash
# Comprehensive health check
./check-services.sh ao-os

# Detailed deployment verification  
./verify-installation.sh ao-observability ao-os

# Real-time monitoring
kubectl get pods -n ao-os -w

# Resource utilization
kubectl top pods -n ao-os
kubectl top nodes

# Event analysis
kubectl get events -n ao-os --sort-by=.metadata.creationTimestamp
```

#### Advanced Diagnostics
```bash
# Prometheus configuration validation
kubectl exec -n ao-os prometheus-0 -- promtool config check /etc/prometheus/prometheus.yml

# Loki configuration validation
kubectl exec -n ao-os loki-0 -- /usr/bin/loki -config.file=/etc/loki/local-config.yaml -verify-config

# Grafana datasource testing
kubectl exec -n ao-os grafana-xxx -- curl -s http://prometheus:9090/api/v1/query?query=up

# Network connectivity testing
kubectl exec -n ao-os prometheus-0 -- wget -qO- http://node-exporter:9100/metrics | head

# Check deployment status
helm status ao-observability -n ao-os

# List all services with NodePort access
kubectl get svc -n ao-os | grep NodePort
```

