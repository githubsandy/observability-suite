# Default values for helm-kube-observability-stack.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# This will set the replicaset count more information can be found here: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/
replicaCount: 1

# This sets the container image more information can be found here: https://kubernetes.io/docs/concepts/containers/images/
image:
  repository: nginx
  # This sets the pull policy for images.
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

# This is for the secrets for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
# This is to override the chart name.
nameOverride: ""
fullnameOverride: ""

# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# This is for setting Kubernetes Annotations to a Pod.
# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
podAnnotations: {}
# This is for setting Kubernetes Labels to a Pod.
# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
podLabels: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
service:
  # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
  type: ClusterIP
  # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
  port: 80


resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# This is to setup the liveness and readiness probes more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
livenessProbe:
  httpGet:
    path: /
    port: http
readinessProbe:
  httpGet:
    path: /
    port: http

# This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: []

affinity: {}

#*************************************************************
# DYNAMIC OBSERVABILITY STACK CONFIGURATION
# Infrastructure-Agnostic Configuration for Any Environment
#*************************************************************

#*************************************************************
# ENVIRONMENT CONFIGURATION (Override for different deployments)
#*************************************************************
environment:
  # Environment identifier (calo-lab, aws-prod, gcp-staging, etc.)
  name: "calo-lab"
  
  # Target namespace (dynamic - can be any namespace)  
  namespace: "ao"  # CALO Lab uses lowercase 'ao'
  
  # Infrastructure type (kubernetes, docker, bare-metal)
  type: "kubernetes"
  
  # Cluster configuration
  cluster:
    # Node selector strategy (labels, hostname, or none for any node)
    nodeSelector:
      enabled: true
      strategy: "labels"  # Use labels for CALO lab
      # CALO Lab specific labels
      nodeLabels:
        ao-node: observability  # Existing CALO lab node labels
      # Alternative hostname strategy (for other environments)
      targetNodes: []
        # - "uta-k8s-ao-01"
        # - "uta-k8s-ao-02"
    
    # Multi-namespace monitoring capability
    crossNamespaceMonitoring:
      enabled: true
      targetNamespaces:
        - "ao"                # Observability namespace
        - "cxtaf-cxeng"      # CXTAF core engine
        - "cxtaf-cxeng-taa"  # CXTAF TAA
        - "cxtaf-cxeng-ui"   # CXTAF UI
        - "cxtm"             # CXTM framework
        - "default"          # Default namespace
        - "kube-system"      # Kubernetes system
        - "ingress-nginx"    # Ingress controller
        - "longhorn-system"  # Storage system
        - "metallb-system"   # Load balancer
      
    # Storage configuration (CALO lab uses Longhorn)
    storage:
      storageClass: "longhorn-single"  # CALO lab default storage class
      persistence:
        enabled: true
        accessModes: ["ReadWriteOnce"]

#*************************************************************
# COMPONENT ENABLEMENT (Turn components on/off dynamically)
#*************************************************************
components:
  # Core Stack (Always enabled)
  core:
    prometheus: true
    grafana: true 
    loki: true
    promtail: true
  
  # Enhanced Monitoring (New components)
  enhanced:
    # Distributed tracing (direct ingestion, no collector needed)
    tempo: true
    
    # Alert management
    alertmanager: true
    
    # Network monitoring
    smokeping: true
    mtr: true  # My Traceroute for path analysis
    
    # Synthetic monitoring
    playwright: true
    sslMonitoring: true
    dnsMonitoring: true
    
    # Enhanced exporters
    enhancedBlackbox: true
    
    # Container metrics
    cadvisor: true  # Container resource metrics from kubelet
  
  # Database exporters (CALO lab has these)
  databases:
    mongodb: false       # Not detected in CALO lab
    postgresql: false    # Not detected in CALO lab  
    redis: true         # Available in CXTM namespace
    mariadb: true       # Available in CXTM namespace
  
  # Application exporters (conditional) 
  applications:
    jenkins: false      # Not detected in CALO lab
    fastapi: true       # Custom metrics enabled

#*************************************************************
# MONITORING TARGETS (Dynamic endpoint configuration)
#*************************************************************
monitoringTargets:
  # External endpoints to monitor
  external:
    enabled: true
    endpoints:
      - name: "google"
        url: "https://www.google.com"
        module: "http_2xx"
        interval: "30s"
      - name: "github" 
        url: "https://www.github.com"
        module: "http_2xx"
        interval: "30s"
      - name: "calo-ingress"
        url: "http://10.122.28.100"
        module: "http_2xx"
        interval: "15s"
      - name: "cxtaf-frontend"
        url: "https://uta-load-balancer.cisco.com"
        module: "http_2xx"
        interval: "30s"
  
  # Internal services (auto-discovery + manual)
  internal:
    autoDiscovery: true
    services:
      # CXTAF services monitoring
      - namespace: "cxtaf-cxeng"
        service: "cxtaf-grafana-svc"
        port: 80
      - namespace: "cxtaf-cxeng"
        service: "cxtaf-frontend-svc"
        port: 80
      - namespace: "cxtaf-cxeng"
        service: "cxtaf-backend-svc"
        port: 80
      # CXTM services monitoring  
      - namespace: "cxtm"
        service: "cxtm-web"
        port: 8080
      - namespace: "cxtm"
        service: "cxtm-mariadb"
        port: 9104  # Metrics port
      - namespace: "cxtm"
        service: "cxtm-redis"
        port: 9121  # Metrics port
  
  # Network targets for MTR/Smokeping
  network:
    enabled: true
    targets:
      - name: "dns-google"
        host: "8.8.8.8"
        description: "Google DNS"
      - name: "dns-cloudflare"
        host: "1.1.1.1" 
        description: "Cloudflare DNS"
      - name: "calo-ingress-ip"
        host: "10.122.28.100"
        description: "CALO Lab Ingress IP"
      - name: "cxtm-mariadb"
        host: "cxtm-mariadb.cxtm.svc.cluster.local"
        description: "CXTM MariaDB Internal"

#*************************************************************
# RESOURCE ALLOCATION (Dynamic sizing based on environment)
#*************************************************************
resources:
  # Resource sizing strategy (auto-detected based on environment)
  sizing: "medium"  # small, medium, large, xlarge, custom
  
  # Pre-defined sizing templates for different environments
  templates:
    small:    # For development/testing (< 4GB RAM environments)
      cpu: 
        requests: "100m"
        limits: "500m"
      memory:
        requests: "128Mi" 
        limits: "512Mi"
    
    medium:   # For CALO lab and similar environments
      cpu:
        requests: "250m"
        limits: "500m"
      memory:
        requests: "256Mi"
        limits: "512Mi"
    
    large:    # For production environments
      cpu:
        requests: "500m"
        limits: "1000m"
      memory:
        requests: "512Mi"
        limits: "1Gi"
    
    xlarge:   # For high-scale environments
      cpu:
        requests: "1000m"
        limits: "2000m"
      memory:
        requests: "1Gi"
        limits: "2Gi"

#*************************************************************
# ALERTING CONFIGURATION (Dynamic rules)
#*************************************************************
alerting:
  enabled: true
  
  # Alert severity levels
  severity:
    critical:
      thresholds:
        cpu: 90
        memory: 90
        disk: 85
        response_time: 5000  # milliseconds
    
    warning:
      thresholds:
        cpu: 75
        memory: 75
        disk: 70
        response_time: 2000
  
  # Notification channels (configure as needed for each environment)
  notifications:
    slack:
      enabled: false
      webhook: ""
    email:
      enabled: false
      smtp: {}
    webhook:
      enabled: false
      url: ""

#*************************************************************
# SYNTHETIC MONITORING (Dynamic test configuration)
#*************************************************************
synthetic:
  # Web application monitoring
  web:
    enabled: true
    tests:
      - name: "cxtaf-frontend-health"
        url: "https://uta-load-balancer.cisco.com"
        script: "health-check.js"
        interval: "5m"
      - name: "cxtm-web-api"
        url: "http://cxtm-web.cxtm.svc.cluster.local:8080/health"
        type: "api"
        interval: "1m"
  
  # Custom health checks for CALO lab services
  healthChecks:
    enabled: true
    checks:
      - name: "cxtaf-grafana"
        endpoint: "http://cxtaf-grafana-svc.cxtaf-cxeng.svc.cluster.local/api/health"
        interval: "30s"
      - name: "cxtm-mariadb"
        endpoint: "http://cxtm-mariadb.cxtm.svc.cluster.local:3306"
        interval: "30s"

#*************************************************************
# NETWORK MONITORING CONFIGURATION
#*************************************************************
network:
  # Smokeping configuration
  smokeping:
    enabled: true
    probeInterval: "60s"
    samples: 20
  
  # MTR configuration  
  mtr:
    enabled: true
    interval: "300s"  # 5 minutes
    maxHops: 30
    packetSize: 64
  
  # DNS monitoring
  dns:
    enabled: true
    servers:
      - "8.8.8.8"
      - "1.1.1.1"
      - "10.43.0.10"  # CALO lab CoreDNS
    queries:
      - "google.com"
      - "github.com"
      - "uta-load-balancer.cisco.com"
      - "cxtm-web.cxtm.svc.cluster.local"

#*************************************************************
# SECURITY & COMPLIANCE
#*************************************************************
security:
  # SSL/TLS monitoring
  ssl:
    enabled: true
    certificateExpiryDays: 30  # Alert when cert expires within 30 days
    endpoints:
      - "https://uta-load-balancer.cisco.com"
      - "https://www.google.com"
  
  # RBAC configuration
  rbac:
    create: true
    serviceAccountName: "observability-stack"

#*************************************************************
# PERFORMANCE & SCALING
#*************************************************************
performance:
  # Horizontal Pod Autoscaler
  hpa:
    enabled: false  # Disabled for CALO lab (sufficient resources)
    minReplicas: 1
    maxReplicas: 5
    targetCPU: 70
    targetMemory: 80
  
  # Data retention policies
  retention:
    prometheus: "15d"
    loki: "7d"  
    tempo: "7d"
  
  # Scrape intervals (adjust for performance)
  scrapeIntervals:
    default: "15s"
    slow: "60s"
    fast: "5s"

#*************************************************************
# NEW COMPONENTS CONFIGURATION
#*************************************************************

# Grafana Tempo configuration
# Note: Direct trace ingestion without any collector dependencies
# Configured for zero external dependencies
tempo:
  enabled: "{{ .Values.components.enhanced.tempo }}"
  image: "grafana/tempo:2.4.1"
  replicas: 1
  resources:
    limits:
      cpu: "{{ .Values.resources.templates[.Values.resources.sizing].cpu.limits }}"
      memory: "{{ .Values.resources.templates[.Values.resources.sizing].memory.limits }}"
    requests:
      cpu: "{{ .Values.resources.templates[.Values.resources.sizing].cpu.requests }}"
      memory: "{{ .Values.resources.templates[.Values.resources.sizing].memory.requests }}"
  service:
    type: ClusterIP
    port: 3200

# AlertManager configuration
alertmanager:
  enabled: "{{ .Values.components.enhanced.alertmanager }}"
  image: "prom/alertmanager:v0.26.0"
  replicas: 1
  resources:
    limits:
      cpu: "{{ .Values.resources.templates[.Values.resources.sizing].cpu.limits }}"
      memory: "{{ .Values.resources.templates[.Values.resources.sizing].memory.limits }}"
    requests:
      cpu: "{{ .Values.resources.templates[.Values.resources.sizing].cpu.requests }}"
      memory: "{{ .Values.resources.templates[.Values.resources.sizing].memory.requests }}"

# Smokeping configuration
smokeping:
  enabled: "{{ .Values.components.enhanced.smokeping }}"
  image: "linuxserver/smokeping:latest"
  resources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "100m"
      memory: "128Mi"

# MTR configuration
mtr:
  enabled: "{{ .Values.components.enhanced.mtr }}"
  image: "alpine:latest"
  resources:
    limits:
      cpu: "100m"
      memory: "128Mi"
    requests:
      cpu: "50m"
      memory: "64Mi"

#*************************************************************
# Observability stack-specific configurations  
#*************************************************************

# Legacy namespace support (dynamic namespace resolution)
namespace: "{{ .Values.environment.namespace | default \"kube-observability-stack\" }}"

prometheus:
  image: prom/prometheus:v2.45.0
  replicas: 1
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 256Mi
  service:
    type: ClusterIP
    port: 9090

nodeExporter:
  image: prom/node-exporter:v1.6.1
  resources:
    limits:
      cpu: 200m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 64Mi

loki:
  image: grafana/loki:2.9.7
  replicas: 1
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 256Mi
  service:
    type: ClusterIP
    port: 3100

promtail:
  image: grafana/promtail:2.9.7
  resources:
    limits:
      cpu: 200m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 64Mi

grafana:
  image: grafana/grafana:10.0.3
  adminPassword: admin
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 256Mi
  service:
    type: ClusterIP
    port: 3000
  storage:
    size: 10Gi
    # storageClass: ""  # Use default storage class

blackboxExporter:
  resources:
    limits:
      cpu: "500m"
      memory: "128Mi"
    requests:
      cpu: "250m"
      memory: "64Mi"

# Foundation Exporters Configuration
kubeStateMetrics:
  image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
  replicas: 1
  resources:
    limits:
      cpu: 250m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
  service:
    type: ClusterIP
    port: 8080

mongodbExporter:
  image: percona/mongodb_exporter:0.40.0
  replicas: 1
  mongodbUri: "mongodb://username:password@mongodb-host:27017"  # Change this to your MongoDB URI
  resources:
    limits:
      cpu: 200m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 64Mi
  service:
    type: ClusterIP
    port: 9216

postgresExporter:
  image: prometheuscommunity/postgres-exporter:v0.15.0
  replicas: 1
  dataSourceName: "postgresql://username:password@postgres-host:5432/dbname?sslmode=disable"  # Change this to your PostgreSQL connection string
  resources:
    limits:
      cpu: 200m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 64Mi
  service:
    type: ClusterIP
    port: 9187

# Application Layer Exporters Configuration
jenkinsExporter:
  image: lovoo/jenkins_exporter:latest
  replicas: 1
  jenkinsServer: "http://your-jenkins-host:8080"  # Change to your Jenkins URL
  jenkinsUsername: "your-jenkins-username"        # Change to your Jenkins username
  jenkinsPassword: "your-jenkins-password"        # Change to your Jenkins password/token
  resources:
    limits:
      cpu: 200m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 64Mi
  service:
    type: ClusterIP
    port: 9118

redisExporter:
  image: oliver006/redis_exporter:latest
  replicas: 1
  redisAddr: "redis://your-redis-host:6379"       # Change to your Redis address
  redisPassword: ""                                # Add Redis password if required
  resources:
    limits:
      cpu: 200m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 64Mi
  service:
    type: ClusterIP
    port: 9121

fastApiMetrics:
  image: python:3.11-slim                         # Custom FastAPI app with metrics
  replicas: 1
  logLevel: "INFO"
  resources:
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 250m
      memory: 128Mi
  service:
    type: ClusterIP
    appPort: 8000
    metricsPort: 8001

ingress:
  enabled: true
  className: "nginx"  # Change to your ingress class (nginx, traefik, etc.)
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    # nginx.ingress.kubernetes.io/cert-manager-cluster-issuer: "letsencrypt-prod"
  
  # Grafana Ingress Configuration
  grafana:
    enabled: true
    host: grafana.os.com
    path: /
    pathType: Prefix
    tls:
      enabled: false
      secretName: grafana-tls
  
  # Prometheus Ingress Configuration  
  prometheus:
    enabled: true
    host: prometheus.os.com
    path: /
    pathType: Prefix
    tls:
      enabled: false
      secretName: prometheus-tls
  
  # Loki Ingress Configuration
  loki:
    enabled: true
    host: loki.os.com
    path: /
    pathType: Prefix
    tls:
      enabled: false
      secretName: loki-tls
  
  # Blackbox Exporter Ingress Configuration
  blackboxExporter:
    enabled: true
    host: blackbox.os.com
    path: /
    pathType: Prefix
    tls:
      enabled: false
      secretName: blackbox-tls